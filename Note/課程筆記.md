## 自然語言處理 筆記整理

---

### Course 2 - Linguistic Essential

#### Transformer vs. Traditional NLP technology

- BERT 可以有很好的表現，但是其他額外的處理技術可以讓模型有更好的表現

- 雖然 BERT 不需要 Feature Extraction 

  (Segmentation已經不需要了，或者說沒有那麼重要了，因為 Transformer-based Model 是以 Character 為單位來處理字元)

- 但 Feature Extraction 仍然對於模型的優化有幫助

#### Part of Speech (PoS) Tagging 詞性標注

- 名詞、動詞、形容詞
- Open word class vs. Closed word class
  - Open word class: 名詞、動詞、形容詞
  - Closed word class: 連接詞、介系詞、代名詞
  - 哪個比較重要並非絕對，要看應用在哪
- 若我們想將某字的不同型態視為同個字，則需要 Stemming (詞幹提取) 或是 Lemmatization (字根還原)
  - Stemming: 直接去尾
    - 此結果並不保證會得到一個真實存在的字
    - Ex. Decomposing -> Decompos
    - 因此若是要直接呈現給使用者，則此方法不好
    - 但若僅僅是要 Group 某單字，則這個方法可以
  - Lemmatization: 還原為原形
    - 這會得到一個真實存在的字 (原形)
    - Ex. Decomposing -> Decompose
- Determiner 冠詞
  - A 表示前面沒出現過這個字，但 The 表示前面有出現過這個字
- Verb 動詞
  - 基本上是句子中最重要的字
- Adverb 副詞
  - 可以表示地點、時間、態度以及程度
- PoS Tag 有標準的查表，其中 Penn 最常用在 NLP，他將所有詞性分為 45 類
- 不同的 PoS Tagging 結果，會讓句子有不同的解釋方式
- 對於中文來說，PoS會對被、的等字有特別的標注

#### Sentence

- Sentence Segmentation 並非那麼容易，但也沒有那麼重要
- Word order (字詞順序) 非常重要，不同順序會有完全不同的語意
- 因此 Bag of Word 無法有很好的表現，因為他沒辦法考慮到字詞順序的影響
- Syntactic Parsing
  - Rewrite Rule 可以 Parse Sentence，同時建構出 Parse Tree，將句子結構化，並了解句子的語意
  - 然而同一個 Sentence 可能有許多不同的 Parse Tree 
- Dependency Parsing
  - 專注於字詞間的關係
  - 在 NLP 領域，Dependency Parsing 比 Syntactic Parsing 更重要

#### Semantics

- Relationship between words: 同義詞、反義詞等等
- WordNet
- Pragmatics (現今主要仍在研究的方向)
  - 這個詞跟哪個詞有關係，他指的是什麼

#### Text Preprocessing

- Tokenization
  - Python Split function (但他不會移除標點，故不佳)
  - NLTK (pip install nltk)
  - 可以透過 string.punctuation 或是 isalopha 來移除標點或是非字母的字元
  - Stop word: 對任務沒有貢獻的字 (但有時候 stop word 是有用的)
  - 可以透過 nltk 移除 stop word
- Stemming
  - 直接用 nltk snowball (但對於 ran 等 irregular 的字詞，他會失敗)
- Lemmatization
  - There is a dictionary in WordNet
  - nltk 的 WordNetlemmatizer
  - 比 Stemming 更好
- 大部分時候做 Stemming 與 Lemmatization 都是好的，但若任務需要保留時態或單複數，則不要做

---

### Course 3  - Collocation 找出專有名詞

- Term Frequency - 透過 TF-IDF 抓出「重要的詞」
- Collocation to Technical Term - 如何抓出「專有名詞組合」
  - 透過技術文章搭配 Collocation Mining 技術，可以萃取出 Terminology
- Collocation vs. Related word
  - Collocation 表示兩字詞為搭配詞
  - Related Word 表示兩字詞之間有某種關係
- Collocation 的特性
  - 不可拆分
  - 不可替換
  - 不可變動
- Collocation Mining
  - Frequency based - 以「多個字詞常出現在一起」為判斷基礎
    - Bigram (尋找) + POS (過濾)
  - Hypothesis testing - 搭配 Confusion Matrix 檢查某組合是否落在常態分佈的接受域
    - 透過 t-test 與 Chi-Square Test
  - Mutual information - 從機率角度下手
    - I(w1, w2) = log2( P(w1, w2) / P(w1)P(w2) )
    - P(w1, w2) = COUNT(w1, w2) / All Bigrams
    - P(w1) = COUNT(w) / All Unigrams
    - 要設「出現次數Count」的 Threshold，例如至少出現 5 次
    - 都可以從 Bigram 延伸到 Trigram 甚至更多，視應用領域而定
    - 如何 Filter 找出的結果？
      - 把有 Overlap 的都移除
      - 多找一點長字串
  - Mean and Variance - 透過計算兩字詞在文章中出現的距離判斷

---

### Course 4 - Language Model

- Language Model - 某個句子或字詞出現的機率

  - Chain Rule

    - 透過機率與貝氏定理計算單字的出現機率
    - 缺點是機率很容易變成 0，因此要透過取 Log 來避免
    - 取 Log 的優點
      - 避免 Underflow
      - 相加的計算比相乘更快速
    - Unigram Model vs. Bigram Model
      - Unigram 無法考量到順序性
    - 延伸至 Trigram、Four-gram、k-gram Model
    - N-gram Model 的缺點
      - 若 N 過小，則「遠距離的相依會無法被考量到」
      - 若 N 過大，則會缺少長字串的資料

  - NN-based Model

    - NN-based Model 可以有效地捕捉遠距離的相依關係
    - Word Embedding 可以捕捉語義
    - RNN
      - t-1 時間的 output 會成為 t 時間的其中一個 input
    - Transformer
      - GPT-2、GPT-3
      - 用途廣泛

  - Smoothing

    - 如何處理沒看過的字 (OOV)？

    - Add-One (Laplace) Estimation & Add-One Smoothing

      <img src="/Users/john/Desktop/截圖 2020-11-09 下午10.38.18.png" alt="截圖 2020-11-09 下午10.38.18"/>

      - 然而 Add-One Smoothing 會有 Overestimation 的問題

    - Additive (Lidstone) Smoothing

      - Adding a smaller number instead of one (Lambda)
      - <img src="/Users/john/Library/Application Support/typora-user-images/截圖 2020-11-09 下午10.50.36.png" alt="截圖 2020-11-09 下午10.50.36" style="zoom:33%;" />

    - Good Turning Smoothing

      - 用出現 c+1 次的字詞去估計出現 c 次的字詞
      - 如此可以解決上面兩種 Smoothing 的 unseen 機率給太大的問題，以及 impossible n-gram 問題

  - Evaluation

    - Extrinsic Evaluation (外在驗證) - 比較不同模型的表現
    - Intrinsic Evaluation (內在驗證) - 切分 Training 與 Test Data
    - Perplexity (困惑度) - 越小越好

  - Text Classification with Language Model

    - 訓練生成正面評論的 LM，以及生成負面評論的 LM，預測時判斷該句子由哪個 LM 生成的機率比較高

---

### Course 5 - Performance Evaluation & Word Sense Disambiguation

- Performance Evaluation
  - Data
    - Training Data - 參數最佳化
    - Validation Data - 超參數調整
    - Test Data - 最終測試
  - Cross Validation (k-fold)
    - 預防大量資料被浪費去做 Test
    - k 越大，越多資料可以被訓練
    - 相對也要花更多時間進行多輪訓練
    - 常見的 k 為 5 (Five-fold cross-validation is the most popular)
  - Performance Metrics
    - Accuracy (正確率) - 做了多少正確決策
    - Precision (準確率) - 對自己判斷 Positive 的信心程度
    - Recall (召回率) - 真正 Positive 中抓出多少
    - F-score - Precision 與 Recall (PR) 的調和平均 (*2PR / (P+R)*)
    - 指標判讀
      - Accuracy 在面對 Imbalance Data 時會產生問題 (ex. COVID-19)
  - Four cases of Prediction
    - True-Positive
    - True-Negative
    - False-Positive (偽陽性)
    - False-Negative (偽陰性)
  - Sensitivity (敏感度)  &  Specificity (特異度)
    - 常用於資料探勘與醫療領域
    - Sensitivity (敏感度) = Recall = 抓出多少 Positive
    - Specificity (特異度) = Recall of negative cases = 抓出多少 Negative
  - 多類別如何使用這些指標
    - 針對各類別分別計算各指標
    - 也就是轉成 Binary 的角度去計算
  - Confusion Matrix
    - 可以用於多類別分類
    - Macro-Averaging - 直接將各列別的某指標做平均計算，做為整體的指標
    - Micro-Averaging - 根據各類別的數量做加權平均
  - 如何知道兩模型的表現是否相同
    - 1% 表示沒差，10% 表示有差
    - 假設檢定
- Word Sense Disambiguation (WSD) 詞義消岐
  - 同個字詞會有不同解釋
  - Supervised
  - Unsupervised
    - Lesk Algorithm

---

### Course 6 - Text Classification

- Models
  - Naive Bayes Model
    - 基於統計獨立性假設
    - Additive Smoothing (0.001*2)
  - Decision Tree Classifier
    - 優點是「結果可以被解釋」
    - Decision tree 一定要做 Pruning
      - 避免 Overfitting
      - 限制樹高
      - 若樣本過少則不切割出該節點
  - Random Forest
    - 多個 Decision Tree 多數決
    - 隨機抽「不同樣本」與「不同變數」來訓練 Decision Tree
    - 可以避免 Overfitting
    - 但結果就會比較難被解釋
  - Gradient Boosting
    - 按照序列逐漸建立每一棵 Decision Tree
    - 透過添加一個新的 Learner，逐漸改善現有的 Learners
    - GB 通常會比 RF 的結果來得更好
    - 最有名的就是 XGBoost
  - Perceptron
    - 在多維空間中尋找超平面 (Hyperplane)
  - Logistic Regression
    - 透過 Sigmoid (Logistic) Function 把結果對應至 0~1
    - y = 1 / (1 + e^(-z))
  - Neural Network
  - kNN
    - 附近最多的樣本就跟他們同一群
    - 如何計算距離 (相似度)
      - Jaccard Coefficient - 「共同出現的單字」佔「總出現單字」的比例
      - Cosine Similarity
- Feature Extraction
  - Bag-of-Words - 很常用，但缺點是順序無法被考量
  - Word Embedding - 常用於訓練 NN (CNN, RNN...)，相似的字詞會距離較近
  - Sentence Embedding
  - 訓練模型時，可以額外多考量前幾章節提到的語言特性
    - Bag-of-Word 可以延伸至 Bag-of-Bigrams 等
    - 中文必須做 Word Segmentation
    - 將同類型的字詞歸類在一起，賦予字詞 Concept，並透過 BOW 或 NN 去抓出特徵
    - POS 也很好用，可以透過 NN 去取得 POS 結果的有用資訊
  - Input 長度、位置也都是有用的資訊
  - 透過觀察相關性或權重，可以知道哪個字詞對結果的貢獻度最高
- Overfitting
  - 問題可能出在資料或模型
    - 資料太少
    - 模型太複雜
  - 可以透過 Regularization 解決 (其中一種解決方法)
  - Decision Tree 要透過 Pruning，Logistic Regression 則透過 Regularization 解決 Overfitting
  - NN 則要透過減少深度或節點數量，或者透過 Dropout 去解決 Overfitting
  - 有時也可以移除出現次數很少的字詞 (特徵)，透過設定出現次數下限 Threshold 的方式達成
  - 模型考慮的特徵越少，越不會有 Overfitting 的情形發生 (3 Features with 90% Acc is better than 100 Features with 92% Acc)
  - Lasso Regularization vs. Ridge Regularization
    - Lasso (L1)
      - 絕對值
      - 較容易產生 0 係數
    - Ridge (L2)
      - 次方
      - 對權重高的係數給予大量懲罰
      - 然而不太容易讓係數變為 0
    - Elastic-Net
      - 結合 L1 與 L2 的 Regularization Method

---

### Course 7 - Sequence Labeling

- 如何避免 Overfitting
  - 更多訓練資料
  - 降低模型複雜度
  - 特徵選擇（在準確度提升有限之下，變數放越少通常越好）
    - 移除不重要的特徵，例如只出現一次的字等
    - 確認特徵與目標標籤的相關性
    - 透過模型選擇特徵（PCA）
  - Smoothing / Pruning / Regularization (目的為了減少變數的數量與係數的大小)
  - NN: 減少模型層數或節點數、Dropout
- Sigmoid - 將結果 y 對應到 0~1 之間，x 值越大 y 會越趨近 1，越小則趨近 0，且以 x = 0 為中心，其 y 值為 0.5
- Regularization 
  - Lambda 通常給 0.1 左右差不多
  - Lasso Regularization
    - L1 (Manhattan distance)
    - 全部權重的「絕對值」加總
    - 使用時此結果要乘上一 Lambda 值，並加入 Loss 中即可
  - Ridge Regularization
    - L2 (Squared Euclidean distance)
    - 全部權重的「平方」加總
    - 使用時此結果要乘上一 Lambda 值，並加入 Loss 中即可
  - L1 vs. L2
    - L1 對每個係數給予相同比重的懲罰，如此可以讓許多係數變為 0，也就是達成去除某些特徵的目的
    - L2 對較大的係數給予較重的懲罰，但當係數值趨近 0 時，便很難再降低，因此很難真正去除某些特徵
    - L1 微分的結果是一個常數，而 L2 微分的結果是 2 * weight
  - Elastic-Net
    - 同時加上 L1 與 L2 這兩個懲罰項，並給予不同的 Lambda
- Word Classes
  - Open Classes (Function Words): 動詞、名詞、形容詞、副詞
  - Closed Classes (介系詞...)
- POS Tagging (詞性標注)
  - 最常用的詞性集 - Penn Treebank
  - POS 很重要，因為字詞歧義常常發生，因此若是知道詞性，對於語意理解會有幫助
  - 如何決定 POS Tag ?
    - 透過前後文
    - 該字詞「各詞性出現的機率分佈」
  - POS 的應用領域
    - 機器翻譯 - 各字詞的重新排列
    - Text-to-speech - 字詞發音
    - 提供下游 NLP 任務一個參考的基礎
- Sequence Labeling
  - Sequence Labeling 可以想成一個分類任務，其結果取決於周遭的字詞，以及前一個（前面多個）字詞的標注結果
  - Hidden Markov Model (HMM)
    - 狀態轉移
    - 由「觀察值」推得「狀態」
    - HMM 的「當前狀態」，由「當前觀察值」與「前一個狀態」所決定
    - 最後獲得機率最大的狀態序列
    - HMM 證明了上面的 POS 取得方式 - 透過「前後文」 & 「該字詞各詞性出現的機率分佈」決定
    - Decoded by Viterbi Algorithm
  - Conditional Random Fields (CRF)
    - Sequence Labeling 領域最受歡迎的 Model
    - CRF 所得到的結果為 Global Optimal，其結果優於 HMM 與 MEMM
    - CRF 的「當前狀態」，由「整個 x 序列」與「前一個狀態」所決定
    - 此外，CRF 的轉移機率是考量整個狀態序列後的最佳解，因此可以做到全域最佳化
    - CRF 的 Input 可以是各式各樣的特徵
      - Unigram / Bigram / Trigram / Variance
      - 其他特徵（POS / 大小寫 / 型態 / 位置 / 開頭結尾 / Suffix / Prefix...）

---

### Course 8 - Sequence Labeling Cont.

- Information Extraction
  - NER 命名實體識別
    - NER 是 Information Extraction 的一個很重要的子任務
    - NER 得到的結果可以做為後續許多 NLP 相關任務的基礎（如實體關係萃取等）
    - NER 是透過 Sequence Labeling 實現（ex. 醫療對話去識別化）
    - BIO Scheme (Begin, Inside, Outside)
    - NER 的 Input 特徵
      - 當前字元、前後文字元
      - 前面幾個 Label
      - POS Tagging
      - Word Shape (字詞型態)
        - 將字詞轉化為固定格式，去反應字詞的長度、大小寫、是否出現數字等特徵
        - Xxxx, xxx, ####...
  - Low Level (Rule based): 正規匹配與字典規則
  - High Level (Model based): RNN
- Chinese Word Segmentation (中文斷詞)
  - 中文斷詞本身就是語意模糊的
  - 很多斷詞方式都是「合理」的，但依據上下文語義，才能得知「正確」的斷詞方式
    - 日/文章/魚/怎麼/說  vs.  日文/章魚/怎麼/說
  - Word Segmentation 也是透過 Sequence Labeling 實現（BIO）
  - Word Segmentation Tagging Scheme - BIO / BME / LRMS
  - CRF 很重要
  - 評估指標 - F1 Score = 2PR / (P+R)
  - 英文 NLP 通常是 Word Based；然而中文 NLP 若是用 Word Based， 則會有 Overfitting 與 OOV 的問題
  - 因此中文現在大多採取 Character Based 的 Model
  - Character Based 在 Deep Learning 的應用中，比 Word Based 的表現更好
  - 且 BERT 是 Character Based 的 Model
  - 中文斷詞套件
    - Jieba
    - CKIP
    - Stanford CoreNLP
- Deep Neural Network 處理 Sequence Labeling
  - RNN
  - Bi-Directional RNN
    - 同時捕捉向前與向後的語義訊息
  - BERT
    - 用一串 Token 當作 Input，去預測各個 Token 的 Output Label
    - Token 可以是 Word (English) 或是 Character (Chinese)
    - POS Tagging 可以當作其中一個特徵
    - 斷詞結果也能當作其中一個特徵
    - 其他萃取的資訊也能當作特徵
  - BERT + Bi-RNN (Bi-LSTM) + CRF 在 Sequence Labeling 相關任務上，可以得到很好的結果

---

### Course 9 - Word Embeddings

- One-hot Representation (Bag-of-word)
  - 字詞之間獨立且無關
- Word Embedding
  - 向量較短
  - 泛化能力強
  - 可以捕捉語義特徵（相似、類比...）
  - Pre-trained Word Embedding
    - Word2Vec (常用)
      - CBOW - 用「附近字詞」來預測「當前字詞」
      - Skip-gram - 用「當前字詞」來預測「附近字詞」
      - Word2Vec 基本想法
        - 將每個字詞轉變成維度在 50~1000 之間的向量
        - 訓練一個預測字詞的分類模型
        - 非監督式學習
    - GloVe (少用)
  - PCA 與 Scatter Plot  可以視覺化字詞間的關係
  - Word Embedding 會學到刻板印象
  - 多語言的 Word Embedding 會將「意義接近」的「不同語言詞語」放在相近的位置
- Word Embedding 應用
  - 如何表示一篇文章，或一個句子的 Embedding?
    - Sum
    - Weighted Sum
  - Sentence Embedding
    - ELMO
    - BERT
    - XLNET

---

### Course 10 - Neural Networks for NLP

- Activation Function
  - Sigmoid
    - 會導致梯度消失
    - 常用於 Output Layer
  - ReLU
    - 會快速收斂，表現佳
    - 常用於 Hidden Layer
  - Tanh
  - Softmax
- CNN
  - 1-D CNN in NLP
  - 1-D vs. 2-D Convolution
  - Max Pooling
  - Input Padding
  - CNN 無法捕捉長距離的相依關係
- RNN
  - Bidirectional 就是疊加兩層，一層向前一層向後
  - Simple RNN
    - t-1 的 Output 與 t 的 Input，通過 tanh，得到 t 時間的 Output
  - LSTM
    - 用更多參數來記住更長遠的資訊
    - 參數量是 Simple RNN 的 4 倍
  - GRU
    - 比 LSTM 簡單且更快速
    - 表現與 LSTM 差不多
    - 在近代被廣泛使用
  - Attention
    - 考慮所有步驟的輸出
- 使用 Neural Network 處理 Document
  - NN 的 Input 最多大概只能處理到 100~200 個字
  - Hierarchical NN Models
    - 底層用多個 RNN (or CNN) 捕捉各區段的特徵 (Sentence Encoder)，再用一個 RNN 統整整篇文章的特徵
      - Pre-trained Sentence Encoder 可以作為各種 NLP 任務中，底層萃取特徵的工具
- ELMo
  - 基於 Bi-LSTM
  - Semi-supervised Learning
- BERT
  - 基於 Transformer
  - BERT 是一種 Sentence Encoder
  - Multilingual 多語言都投射到同個向量空間
  - 中文是 Character-based
  - 同個字（詞）會因為前後文不同，而有不同的 Embedding
  - 維度為 768 * N
  - BERT 的 Pre-training 方式
    - 克漏字
    - 預測下個句子
    - 兩個任務是同時一起訓練的
  - BERT 的結果可以直接拿來做為後續高階 NLP 任務的底層特徵
  - 然而根據不同任務，Fine-tuning BERT 通常可以有更好的表現
  - 套件資源
    - Keras
    - Pytorch
    - Huggingface
    - Simpletransformers
- Transformer vs. Bi-LSTM (BERT vs. ELMo)
  - Transformer 每一個時間點都可以取得「各時間點的資訊」
  - Bi-LSTM 每一個時間點只能取得「附近時間點的資訊」
- NLP 領域的四大基本任務
  - 單一句子分類
  - 兩句子關聯辨認
  - 序列標注
  - Question-Answering
- NN 如何避免 Overfitting
  - Dropout
  - Early Stopping

---

### Course 11 - Semi-Supervised Approaches to NLP

- Self-Learning

  - 透過標註好的資料，邊訓練模型，邊預測未知資料，並將預測結果作為新資料

- Auto-Encoder

  - Unsupervised Learning
  - Auto-Encoder 可以做為 Sentence Embedding Model
  - Input Sentence & Output Sentence，中間壓縮的部分就是 Sentence Embedding

- Data Augmentation

  - 替換句子裡面的某些字詞
  - 透過 Back Translation (英翻中，再翻回英) 可以做到 Data Augmentation

- Cross-lingual Transfer Learning with MT

  - 透過「機器翻譯」，把資料量多的語言翻譯成資料量少的語言，就能生成新的 Labeled Data

- Machine Translation 機器翻譯，可以很有效的生成資料

---

> Copyright © Yu-Jun Kuo